# src/agent_monitor_playground/agent/loop.py
"""
Agent execution loop.

This module provides the smallest possible “real” agent interface:
- One prompt goes in.
- One model call happens.
- One text output comes out.
- Lightweight metadata is returned for logging and monitoring.

It intentionally does NOT:
- Perform any logging.
- Run any monitors.
- Handle any UI concerns.
Those responsibilities belong to the pipeline layer.
"""

from __future__ import annotations

from dataclasses import dataclass
from typing import Any, Dict, Optional

from agent_monitor_playground.agent.client import OpenAIClient


@dataclass(frozen=True)
class OneShotResult:
    """
    Structured result of a single agent execution.

    This object exists to replace loosely structured dictionaries with a
    stable, typed interface. It makes logging, monitoring, and future
    extensions easier and safer.

    Fields:
    - text:
        The final text generated by the model.
    - model:
        The model name that produced the output.
    - latency_ms:
        Wall-clock time spent waiting for the model, in milliseconds.
    - usage:
        Optional token usage or cost metadata if provided by the client.
    """
    text: str
    model: str
    latency_ms: int
    usage: Optional[Dict[str, Any]] = None


def run_one_shot(
    *,
    prompt: str,
    model: str,
    max_output_tokens: int,
) -> OneShotResult:
    """
    Executes the smallest possible agent interaction.

    This function represents a single, atomic agent step:
    - It does exactly one call to a language model.
    - It does not loop, invoke tools, write logs, or apply monitors.

    Inputs:
    - prompt:
        The full task prompt text.
    - model:
        The model identifier to use (e.g. "gpt-4o-mini").
    - max_output_tokens:
        Upper bound on how many tokens the model may generate.

    Output:
    - OneShotResult containing:
        - Generated text
        - Model identifier
        - API call latency
        - Optional token usage metadata

    Architectural role:
    This function forms the boundary between:
        Agent orchestration  →  Model client  →  External API

    The pipeline layer is responsible for:
    - Logging before and after this call.
    - Passing results to monitors.
    - Aggregating reports.
    """

    # Create a client instance.
    # This is cheap and stateless; it only wraps the OpenAI SDK.
    client = OpenAIClient()

    # Execute a single prompt → response interaction.
    # The client returns a plain dictionary so fields can evolve
    # without breaking the interface.
    result = client.run_one_shot(
        prompt=prompt,
        model=model,
        max_output_tokens=max_output_tokens,
    )

    # Normalize the raw dictionary into a strongly typed result object.
    # This ensures consistent structure throughout the system.
    return OneShotResult(
        text=result.get("text", ""),
        model=result.get("model", model),
        latency_ms=int(result.get("latency_ms", 0)),
        usage=result.get("usage"),
    )
